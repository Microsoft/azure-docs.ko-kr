---
title: '모델 평가: 모듈 참조'
titleSuffix: Azure Machine Learning
description: Azure Machine Learning에서 모델 평가 모듈을 사용 하 여 학습 된 모델의 정확도를 측정 하는 방법에 대해 알아봅니다.
services: machine-learning
ms.service: machine-learning
ms.subservice: core
ms.topic: reference
author: likebupt
ms.author: keli19
ms.date: 02/11/2020
ms.openlocfilehash: 5951c6ec63478b4b266f22eaf8bf3162e0a45df0
ms.sourcegitcommit: b95983c3735233d2163ef2a81d19a67376bfaf15
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 02/11/2020
ms.locfileid: "77137540"
---
# <a name="evaluate-model-module"></a>모델 평가 모듈

이 문서에서는 Azure Machine Learning designer (미리 보기)의 모듈을 설명 합니다.

이 모듈을 사용 하 여 학습 된 모델의 정확도를 측정 합니다. 모델에서 생성 된 점수가 포함 된 데이터 집합을 제공 하 고 **모델 평가** 모듈은 산업 표준 평가 메트릭 집합을 계산 합니다.
  
 **평가 모델** 에서 반환 되는 메트릭은 평가 하는 모델 유형에 따라 달라 집니다.  
  
-   **분류 모델**    
-   **회귀 모델**    


> [!TIP]
> 모델 평가를 처음 접하는 경우 EdX에서 [기계 학습 과정](https://blogs.technet.microsoft.com/machinelearning/2015/09/08/new-edx-course-data-science-machine-learning-essentials/) 의 일부로 Stephen Elston에 의해 비디오 시리즈를 권장 합니다. 


**모델 평가** 모듈을 사용 하는 방법에는 세 가지가 있습니다.

+ 학습 데이터에 대 한 점수를 생성 하 고 이러한 점수에 따라 모델을 평가 합니다.
+ 모델에 점수를 생성 하지만 이러한 점수를 예약 된 테스트 집합의 점수와 비교 합니다.
+ 동일한 데이터 집합을 사용 하 여 서로 다른 두 개의 관련 모델에 대 한 점수를 비교 합니다.

## <a name="use-the-training-data"></a>학습 데이터 사용

모델을 평가하려면 입력 열 및 점수 집합이 포함된 데이터 집합에 연결해야 합니다.  다른 데이터를 사용할 수 없는 경우 원래 데이터 집합을 사용할 수 있습니다.

1. [점수 매기기 모델](./score-model.md) 의 **점수가 매겨진 데이터 집합** 출력을 **모델 평가**의 입력에 연결 합니다. 
2. **모델 평가** 모듈을 클릭 하 고 파이프라인을 실행 하 여 평가 점수를 생성 합니다.

## <a name="use-testing-data"></a>테스트 데이터 사용

기계 학습에서 일반적인 시나리오는 [분할](./split-data.md) 모듈 또는 [파티션 및 샘플](./partition-and-sample.md) 모듈을 사용 하 여 원래 데이터 집합을 학습 및 테스트 데이터 집합으로 분리 하는 것입니다. 

1. [점수 매기기 모델](score-model.md) 의 **점수가 매겨진 데이터 집합** 출력을 **모델 평가**의 입력에 연결 합니다. 
2. 테스트 데이터를 포함 하는 분할 데이터 모듈의 출력을 **모델 평가**의 오른쪽 입력에 연결 합니다.
2. **모델 평가** 모듈을 클릭 하 고 **선택 된 실행** 을 선택 하 여 평가 점수를 생성 합니다.

## <a name="compare-scores-from-two-models"></a>두 모델의 점수 비교

두 번째 점수 집합을 연결 하 여 모델을 **평가할**수도 있습니다.  점수는 동일한 데이터에 대 한 다른 모델의 결과 집합 또는 알려진 결과가 있는 공유 평가 집합 일 수 있습니다.

이 기능은 동일한 데이터에서 서로 다른 두 모델의 결과를 쉽게 비교할 수 있으므로 유용합니다. 또는 서로 다른 매개 변수를 사용하여 동일한 데이터에 대해 수행한 두 번의 실행 결과 생성된 점수를 비교할 수 있습니다.

1. [점수 매기기 모델](score-model.md) 의 **점수가 매겨진 데이터 집합** 출력을 **모델 평가**의 입력에 연결 합니다. 
2. 모델 점수 매기기 모듈의 출력을 **모델 평가**의 오른쪽 입력에 연결 합니다.
3. 파이프라인을 실행합니다.

## <a name="results"></a>결과

**모델 평가**를 실행 한 후 모듈을 마우스 오른쪽 단추로 클릭 하 고 **평가 결과 시각화** 를 선택 하 여 결과를 확인 합니다.

**모델 평가**의 두 입력에 데이터 집합을 연결 하는 경우 결과에는 두 데이터 집합 또는 두 모델 모두에 대 한 메트릭이 포함 됩니다.
왼쪽 포트에 연결 된 모델이 나 데이터가 먼저 보고서에 표시 된 다음 데이터 집합에 대 한 메트릭 또는 올바른 포트에 연결 된 모델이 표시 됩니다.  

예를 들어 다음 이미지는 동일한 데이터를 기반으로 하지만 다른 매개 변수를 사용 하는 두 클러스터링 모델의 결과 비교를 나타냅니다.  

![AML&#95;Comparing2Models](media/module/aml-comparing2models.png "AML_Comparing2Models")  

이는 클러스터링 모델 이므로 평가 결과는 두 회귀 모델의 점수를 비교 하거나 두 개의 분류 모델을 비교 하는 경우와 다릅니다. 그러나 전체 프레젠테이션은 동일 합니다. 

## <a name="metrics"></a>메트릭

이 섹션에서는 **모델 평가**에서 사용 하도록 지원 되는 특정 유형의 모델에 대해 반환 되는 메트릭에 대해 설명 합니다.

+ [분류 모델](#metrics-for-classification-models)
+ [회귀 모델](#metrics-for-regression-models)

### <a name="metrics-for-classification-models"></a>분류 모델에 대 한 메트릭

분류 모델을 평가할 때 다음과 같은 메트릭이 보고 됩니다. 모델을 비교 하는 경우 평가를 위해 선택한 메트릭으로 순위가 매겨집니다.  
  
-   **정확도** 는 전체 사례에 대 한 실제 결과의 비율로 분류 모델의 적합도를 측정 합니다.  
  
-   **전체 자릿수** 는 모든 긍정 결과에 대 한 실제 결과의 비율입니다.  
  
-   **회수** 는 모델에서 반환 하는 모든 올바른 결과의 비율입니다.  
  
-   **F-점수** 는 전체 자릿수의 가중치가 적용 된 평균으로 계산 되며, 0과 1 사이의 값이 가장 좋습니다. 여기서 이상적인 F 점수 값은 1입니다.  
  
-   **Cc** 는 y 축에서 진정한 긍정을 사용 하 여 그린 곡선 아래의 면적을 측정 하 고 x 축에는 가양성을 측정 합니다. 이 메트릭은 여러 유형의 모델을 비교할 수 있는 단일 숫자를 제공 하기 때문에 유용 합니다.  
  
- **평균 로그 손실은** 잘못 된 결과에 대 한 패널티를 표현 하는 데 사용 되는 단일 점수입니다. 두 확률 분포 (true)와 모델에 있는 분포의 차이로 계산 됩니다.  
  
- **학습 로그 손실은** 임의 예측을 통해 분류자의 장점을 나타내는 단일 점수입니다. 로그 손실은 출력의 확률을 레이블의 알려진 값 (그라운드 참)과 비교 하 여 모델의 불확실성을 측정 합니다. 전체적으로 모델에 대 한 로그 손실을 최소화 하려고 합니다.

### <a name="metrics-for-regression-models"></a>회귀 모델에 대 한 메트릭
 
회귀 모델에 대해 반환 되는 메트릭은 오류 양을 예측 하도록 디자인 되었습니다.  관찰 된 값과 예측 값의 차이가 적으면 모델은 데이터 웰에 맞게 고려 됩니다. 그러나 잔차의 패턴을 살펴보면 (한 예측 지점과 해당 하는 실제 값 간의 차이) 모델의 잠재적 바이어스에 대해 많은 정보를 확인할 수 있습니다.  
  
 회귀 모델 평가에 대해 다음과 같은 메트릭이 보고 됩니다. 모델을 비교할 때 평가를 위해 선택한 메트릭으로 순위가 매겨집니다.  
  
- **MAE (절대 평균 오차)** 는 실제 결과에 대 한 예측의 종료 방법을 측정 합니다. 따라서 점수가 낮을수록 좋습니다.  
  
- **RMSE (제곱 평균 제곱 오차)** 는 모델에서 오류를 요약 하는 단일 값을 만듭니다. 메트릭은 차이를 제곱 하 여 오버 예측과 예측에서의 차이를 무시 합니다.  
  
- **상대 절대 오차 (RAE)** 는 예상 값과 실제 값의 상대적 절대 차이입니다. 평균 차이는 산술 평균으로 나뉩니다.  
  
- **RSE (상대 제곱 오차** ) 마찬가지로 실제 값의 총 제곱 오차로 나누어 예측 값의 총 제곱 오차를 표준화 합니다.  
  

  
- 일반적으로 R<sup>2</sup>라고도 하는 **결정 계수**는 모델의 예측 능력을 0에서 1 사이의 값으로 나타냅니다. 0은 모델이 무작위로 사용 됨을 의미 합니다 (아무 것도 설명 하지 않음). 1은 완벽 한 일치를 의미 합니다. 그러나 낮은 값은 완전히 정상이 고 높은 값은 주의 대상이 될 수 있으므로 R<sup>2</sup> 값을 해석 하는 데 주의를 기울여야 합니다.
  

## <a name="next-steps"></a>다음 단계

Azure Machine Learning [사용할 수 있는 모듈 집합](module-reference.md) 을 참조 하세요. 